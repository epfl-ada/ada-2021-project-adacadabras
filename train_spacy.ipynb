{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports PIL module \n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = './data/data_nlp/'\n",
    "AD1_FILE = PATH_DATA + 'MeTooHate.csv'\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "df = pd.read_csv(AD1_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(803638, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['status_id', 'created_at', 'favorite_count', 'retweet_count',\n",
    "       'location', 'followers_count', 'friends_count', 'statuses_count',\n",
    "       ], inplace=True)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        0\n",
       "category    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "punct = string.punctuation\n",
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_url(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r\"<.*?>\")\n",
    "    return html.sub(r\"\", text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r\"\", text)\n",
    "\n",
    "def text_data_cleaning(sentence):\n",
    "    sentence = remove_url(sentence)\n",
    "    sentence = remove_html(sentence)\n",
    "    sentence = remove_emoji(sentence)\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            temp = token.lemma_.lower().strip()\n",
    "        else:\n",
    "            temp = token.lower_\n",
    "        tokens.append(temp)\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in nlp.Defaults.stop_words and token not in punct:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yep',\n",
       " 'like',\n",
       " 'triffele',\n",
       " 'woman',\n",
       " 'weaponize',\n",
       " 'poon',\n",
       " 'wonder',\n",
       " 'kamala',\n",
       " 'harris',\n",
       " 'extort',\n",
       " 'willy',\n",
       " 'brown',\n",
       " 'throw',\n",
       " 'poon',\n",
       " 'oh',\n",
       " 'yeh',\n",
       " 'job',\n",
       " 'joke']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data_cleaning(df.text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yep just like triffeling women weaponized their poon!! Wonder if Kamala Harris ever extorted Willy Brown after throwing the poon on him, oh yeh, that how she got her first job me too is a JOKE! '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Isn't it nice that you know everything?       \\n      \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(df.text[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TFIDF***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=text_data_cleaning)\n",
    "classifier = LinearSVC(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text\n",
    "y = df.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((642910,), (160728,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"so... abusing women??? I guess karma is a bitch, huh!!! Remember your mantra...!!  So, those rules should serve you well, right??!! Good luck Creepy porn lawyer.. couldn't have happened to a nicer guy!! JERK\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[524689]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing tfidf, total=111.5min\n",
      "[LibLinear].....*\n",
      "optimization finished, #iter = 55\n",
      "Objective value = -106844.563668\n",
      "nSV = 298712\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=  14.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function text_data_cleaning at 0x7f8dbb262280>)),\n",
       "                ('clf', LinearSVC(verbose=True))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97    141532\n",
      "           1       0.88      0.59      0.71     19196\n",
      "\n",
      "    accuracy                           0.94    160728\n",
      "   macro avg       0.91      0.79      0.84    160728\n",
      "weighted avg       0.94      0.94      0.94    160728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[140001,   1531],\n",
       "       [  7799,  11397]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[X_train[524689]]\n",
    "clf.predict(['love women'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = './data/data_nlp/classifier.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Entitled, obnoxious, defensive, lying weasel. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you  and  for what you did for the women...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Knitting (s) &amp;amp; getting ready for January 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yep just like triffeling women weaponized thei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No, the President wants to end  movement posin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807169</th>\n",
       "      <td>Let’s not forget that this “iconic kiss” was u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807170</th>\n",
       "      <td>DEFINITELY....the only one any of us should su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807171</th>\n",
       "      <td>Did the  movement count the dollars of Erin An...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807172</th>\n",
       "      <td>This is one of my all time fav songs &amp;amp; vid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807173</th>\n",
       "      <td>I watched your news on the death of the sailo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>803638 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  category\n",
       "0       Entitled, obnoxious, defensive, lying weasel. ...         0\n",
       "1       Thank you  and  for what you did for the women...         0\n",
       "2       Knitting (s) &amp; getting ready for January 1...         0\n",
       "3       Yep just like triffeling women weaponized thei...         1\n",
       "4       No, the President wants to end  movement posin...         0\n",
       "...                                                   ...       ...\n",
       "807169  Let’s not forget that this “iconic kiss” was u...         0\n",
       "807170  DEFINITELY....the only one any of us should su...         0\n",
       "807171  Did the  movement count the dollars of Erin An...         0\n",
       "807172  This is one of my all time fav songs &amp; vid...         1\n",
       "807173   I watched your news on the death of the sailo...         0\n",
       "\n",
       "[803638 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***PREDICTIONS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "################################################### Keywords for data filtering #########################################\n",
    "# Talking about women, general women terminology\n",
    "general_dict = set(['woman', 'women', ' she ', ' she.', ' her ', ' her.', 'girl', 'daughter', 'mother',\n",
    "                    'sister', 'niece', 'female', 'wife', 'spouse', 'mistress', ' aunt ', ' aunt.', ' aunts ', ' aunts.',\n",
    "                    ' mom ', ' mom.', ' moms ', ' moms.', ' mum ', ' mum.', ' mums ', ' mums.', ' femme ', ' femme.', 'grandma',\n",
    "                    ' lady ', ' lady.', ' ladies ', ' ladies.', ' panty ', ' panty.', ' panties ', ' panties.',\n",
    "                    'madam', ' ms.', ' mrs.', ' ms ', ' mrs ', ' maid ', ' maid.', ' maids ', ' maids.', ' bride ', ' bride.', ' brides ',\n",
    "                    ' brides.', ' chick ', ' bridesmaid ', ' bridesmaid.', ' bridesmaids ', ' bridesmaids.',\n",
    "                    ' chick.', ' chicks ', ' chicks.', \" she's \", \" her's \"\n",
    "                    ])\n",
    "\n",
    "# Feminine-biased nouns and substrings (Disclaimer: this doesn’t reflect our team’s gender views.)\n",
    "adj_dict = set(['slut', 'gold digger', 'bitch', 'prostitut', 'bimbo', 'actress',\n",
    "                ' queen ', ' queen. ', ' queens ', ' queens. ', 'princess', 'whore', ' loca ', 'goddess', 'maiden', ' loca.'\n",
    "                ' petite ', ' petite.', ' petites ', ' petites.', 'duchess', 'lesbian', 'fashionista', 'doll',\n",
    "                'nymph', 'cougar', 'milf', 'virgin'])\n",
    "\n",
    "# Verbs related to women\n",
    "verb_dict = set(['marri', 'sleep with', 'marry', 'abortion', 'birth control'])\n",
    "\n",
    "# Terms/actions associated both to MeToo movement and women’s datasets\n",
    "action_dict = set(['harrass', ' rape ',' rape.',' raped ',' raped.', ' rapes ', ' rapes.', ' rapist ', ' rapist.', ' raping ', ' raping.',\n",
    "                   'sex', 'domestic violence', 'domestic abuse', 'misogyn'])\n",
    "\n",
    "# MeToo dictionary\n",
    "metoo_dict = set(['metoo', 'femin', 'feminism', 'feminist'])\n",
    "\n",
    "# Personnality MeToo dictionnary\n",
    "people_dict = set(['harvey weinstein', 'bill cosby', 'tarana burke', 'ambra gutierrez',\n",
    "                   'anastasia melnichenko', 'alyssa milano', 'r. kelly', 'r kelly','rob kelly', 'robert kelly',\n",
    "                   'larry nassar', 'reith raniere', 'allison mack',\n",
    "                   'claude arnault'])\n",
    "\n",
    "keywords = general_dict\\\n",
    "    .union(adj_dict)\\\n",
    "    .union(verb_dict)\\\n",
    "    .union(action_dict)\\\n",
    "    .union(metoo_dict)\\\n",
    "    .union(people_dict)\n",
    "\n",
    "################################################# Helper functions ############################################\n",
    "def generate_data_keyword(src_path, dst_path, keywords):\n",
    "    with bz2.open(src_path, 'rb') as s_file:\n",
    "        with bz2.open(dst_path, 'wb') as d_file:\n",
    "            for instance in s_file:\n",
    "                instance = json.loads(instance)\n",
    "                quote = str(instance['quotation']).lower()\n",
    "                for word in keywords:\n",
    "                    if word in f' {quote} ':\n",
    "                        d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))\n",
    "                        break\n",
    "\n",
    "\n",
    "def generate_data_monthly(src_path, dst_path, keywords):\n",
    "    with bz2.open(path_to_quotes, 'rb') as s_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance)\n",
    "            month = instance['date'][5:7]\n",
    "            path_per_month = dst_path.format(month)\n",
    "            with bz2.open(path_per_month, 'wb') as d_file:\n",
    "                d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))\n",
    "\n",
    "\n",
    "def generate_pickles(scr_path, dst_path, chunk_size=1e5):\n",
    "    with bz2.open(dst_path, 'wb') as f:\n",
    "        data_reader = pd.read_json(scr_path, lines=True, compression='bz2', chunksize=chunk_size)\n",
    "        for chunk in data_reader:\n",
    "            pkl.dump(chunk, f)\n",
    "\n",
    "\n",
    "def get_unique_list(serie):\n",
    "    '''\n",
    "    find unique element, and corresponding index, of a Serie with values of type List()\n",
    "\n",
    "    input  : [serie]  : Serie with list for value (might be empty list\n",
    "    output : [unique] : List of unique element in the Serie serie\n",
    "             [idx]    : index of 1st row of serie containing the corresponding [unique] value\n",
    "    '''\n",
    "    unique = []\n",
    "    idx = []\n",
    "    for iiter, i in enumerate(serie.values):\n",
    "        for j in i:\n",
    "            if not (j in unique):\n",
    "                unique.append(j)\n",
    "                idx.append(iiter)\n",
    "    return unique, idx\n",
    "\n",
    "\n",
    "def get_week(dataframe, col):\n",
    "    return dataframe[col].apply(lambda x: datetime.fromisoformat(x[:-7]).isocalendar()[1])\n",
    "\n",
    "\n",
    "def get_month(dataframe, col):\n",
    "    return dataframe[col].apply(lambda x: int(x[5:7]))\n",
    "\n",
    "\n",
    "def split_quotes_per_gender(chunk, df_selected_parquet, qid_male, qid_female, qids_others, qids_wrong):\n",
    "    chunk['week']   = get_week(chunk, 'quoteID')\n",
    "    chunk['month']  = get_month(chunk, 'quoteID')\n",
    "    \n",
    "    #_________________\n",
    "    # SPEAKER NONE\n",
    "    #`````````````````\n",
    "    q_is_speaker_None = chunk.speaker=='None' # Checker auusi pou les NaN -> isna()\n",
    "    q_noSpeaker       = chunk[ q_is_speaker_None].copy()\n",
    "    q_speaker         = chunk[-q_is_speaker_None]\n",
    "    q_speaker['qid'] = q_speaker.qids.apply(lambda x: x[0]) # 1st homonym\n",
    "    q_speaker = q_speaker.drop(columns=['qids'])\n",
    "    \n",
    "    \n",
    "    # Merge with Parquet\n",
    "    q_speaker = q_speaker.merge(df_selected_parquet, left_on='qid', right_on='id', how='left')\n",
    "    \n",
    "    #______________\n",
    "    # SPEAKER NO PARQUET\n",
    "    #``````````````\n",
    "    q_speaker_not_in_parquet = q_speaker.id.isna()\n",
    "    q_speaker_noParquet  = q_speaker[q_speaker_not_in_parquet].copy()\n",
    "    \n",
    "    q_speaker  = q_speaker[-q_speaker_not_in_parquet]\n",
    "    \n",
    "    '''\n",
    "    #__________________\n",
    "    # SPEAKER NO LABEL\n",
    "    #``````````````````\n",
    "    q_is_speaker_labeled = q_speaker.qid.isin(df_qid.QID)\n",
    "    q_speaker_noLabel = q_speaker[ -q_is_speaker_labeled]\n",
    "    q_speaker = q_speaker[ q_is_speaker_labeled]\n",
    "    '''\n",
    "    \n",
    "    #______\n",
    "    # NONE\n",
    "    #``````\n",
    "    q_gender_None = q_speaker.gender.isna()\n",
    "    q_None    = q_speaker[ q_gender_None].copy()\n",
    "    q_speaker = q_speaker[-q_gender_None]\n",
    "    \n",
    "    q_speaker['gender'] = q_speaker.gender.apply(lambda x: x[0]) # keep only 1st gender\n",
    "    \n",
    "    #________________________________\n",
    "    # MALE - FEMALE - OTHERS - WRONG - NOLABEL\n",
    "    #````````````````````````````````\n",
    "    q_is_gender_labeled = q_speaker.gender.isin(df_qid.QID)\n",
    "    q_is_gender_male = q_speaker.gender.isin(qid_male)\n",
    "    q_is_gender_female = q_speaker.gender.isin(qid_female)\n",
    "    q_is_gender_others = q_speaker.gender.isin(qids_others)\n",
    "    q_is_gender_wrong = q_speaker.gender.isin(qids_wrong)\n",
    "    \n",
    "    \n",
    "    q_noLabel  = q_speaker[q_is_gender_labeled].copy()\n",
    "    q_male     = q_speaker[q_is_gender_male].copy()\n",
    "    q_female   = q_speaker[q_is_gender_female].copy()\n",
    "    q_others   = q_speaker[q_is_gender_others].copy()\n",
    "    q_wrong    = q_speaker[q_is_gender_wrong].copy()\n",
    "    \n",
    "    return q_male, q_female, q_others, q_wrong, q_noLabel, q_None, q_speaker_noParquet, q_noSpeaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = './../data/'\n",
    "PATH_PARQUET = PATH_DATA + 'parquet/'\n",
    "df_parquet = pd.read_parquet(PARQUET_FILE)\n",
    "df_parquet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[ Malia ] knows what she is going to do. They have a plan for her and her family feels comfortable knowing that it's not something unstructured,\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2016 # available: from 2015 to 2020\n",
    "PATH_DATA = './data/data_nlp/'\n",
    "QUOTES_FILE = PATH_DATA + f'quotes-{year}-filtered.json.bz2'\n",
    "CHUNK_SIZE = 500\n",
    "\n",
    "reader = pd.read_json(QUOTES_FILE, lines=True, compression='bz2', chunksize=CHUNK_SIZE, typ='frame')\n",
    "\n",
    "chunks = [] #utile pour plus loins quand on fait le feature extraction\n",
    "i=0\n",
    "for chunk in reader:\n",
    "    df_0 = chunk\n",
    "    break\n",
    "df_0.quotation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020 # available: from 2015 to 2020\n",
    "PATH_DATA = './data/data_nlp/'\n",
    "QUOTES_FILE = PATH_DATA + 'metoo_2019_04.json.bz2'\n",
    "\n",
    "df = pd.read_json(QUOTES_FILE, lines=True, compression='bz2', typ='frame')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z2/v95s75vx1flf254b1kb3nfdw0000gn/T/ipykernel_19979/4080023279.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_new['Hatred'] = df_new.quotation.apply(lambda quote: clf.predict([quote]))\n"
     ]
    }
   ],
   "source": [
    "df_new['Hatred'] = df_new.quotation.apply(lambda quote: clf.predict([quote]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_new[df_new.Hatred == 1].quotation.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thankfully, with places like Optimism Place, we have supports put in place for people who are involved, or who are victims or survivors of domestic violence and we have the ability to help those people through those problems -- not only with the survivors, but with those families as well in helping to prevent any further problems from happening.'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split(text, n=100):\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thankfully, with places like Optimism Place, we have supports put in place for people who are involv',\n",
       " 'ed, or who are victims or survivors of domestic violence and we have the ability to help those peopl',\n",
       " 'e through those problems -- not only with the survivors, but with those families as well in helping ',\n",
       " 'to prevent any further problems from happening.']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_s = [s[100*i:100*(i+1)] for i in range(len(s)//100)] + [s[100*(len(s)//100):]];s_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in s_s:\n",
    "    clf.predict([s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(s_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_json(compression='bz2',path_or_buf='./data/data_nlp/TEST.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9904</th>\n",
       "      <th>9905</th>\n",
       "      <th>9906</th>\n",
       "      <th>9907</th>\n",
       "      <th>9908</th>\n",
       "      <th>9909</th>\n",
       "      <th>9910</th>\n",
       "      <th>9911</th>\n",
       "      <th>9912</th>\n",
       "      <th>9913</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'quoteID': '2019-04-14-028425', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-12-006214', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-01-064419', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-09-075194', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-26-057136', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-15-036313', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-25-060160', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-02-073771', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-28-038981', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-03-005994', 'quotation': ...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'quoteID': '2019-04-03-007529', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-11-009600', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-11-078151', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-01-088752', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-12-052424', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-05-099481', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-19-004730', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-09-015136', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-05-022314', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-07-025206', 'quotation': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 9914 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                0     \\\n",
       "0  {'quoteID': '2019-04-14-028425', 'quotation': ...   \n",
       "\n",
       "                                                1     \\\n",
       "0  {'quoteID': '2019-04-12-006214', 'quotation': ...   \n",
       "\n",
       "                                                2     \\\n",
       "0  {'quoteID': '2019-04-01-064419', 'quotation': ...   \n",
       "\n",
       "                                                3     \\\n",
       "0  {'quoteID': '2019-04-09-075194', 'quotation': ...   \n",
       "\n",
       "                                                4     \\\n",
       "0  {'quoteID': '2019-04-26-057136', 'quotation': ...   \n",
       "\n",
       "                                                5     \\\n",
       "0  {'quoteID': '2019-04-15-036313', 'quotation': ...   \n",
       "\n",
       "                                                6     \\\n",
       "0  {'quoteID': '2019-04-25-060160', 'quotation': ...   \n",
       "\n",
       "                                                7     \\\n",
       "0  {'quoteID': '2019-04-02-073771', 'quotation': ...   \n",
       "\n",
       "                                                8     \\\n",
       "0  {'quoteID': '2019-04-28-038981', 'quotation': ...   \n",
       "\n",
       "                                                9     ...  \\\n",
       "0  {'quoteID': '2019-04-03-005994', 'quotation': ...  ...   \n",
       "\n",
       "                                                9904  \\\n",
       "0  {'quoteID': '2019-04-03-007529', 'quotation': ...   \n",
       "\n",
       "                                                9905  \\\n",
       "0  {'quoteID': '2019-04-11-009600', 'quotation': ...   \n",
       "\n",
       "                                                9906  \\\n",
       "0  {'quoteID': '2019-04-11-078151', 'quotation': ...   \n",
       "\n",
       "                                                9907  \\\n",
       "0  {'quoteID': '2019-04-01-088752', 'quotation': ...   \n",
       "\n",
       "                                                9908  \\\n",
       "0  {'quoteID': '2019-04-12-052424', 'quotation': ...   \n",
       "\n",
       "                                                9909  \\\n",
       "0  {'quoteID': '2019-04-05-099481', 'quotation': ...   \n",
       "\n",
       "                                                9910  \\\n",
       "0  {'quoteID': '2019-04-19-004730', 'quotation': ...   \n",
       "\n",
       "                                                9911  \\\n",
       "0  {'quoteID': '2019-04-09-015136', 'quotation': ...   \n",
       "\n",
       "                                                9912  \\\n",
       "0  {'quoteID': '2019-04-05-022314', 'quotation': ...   \n",
       "\n",
       "                                                9913  \n",
       "0  {'quoteID': '2019-04-07-025206', 'quotation': ...  \n",
       "\n",
       "[1 rows x 9914 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF = pd.read_json('./data/data_nlp/TEST.json', lines=True, compression='bz2', typ='frame')\n",
    "DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9904</th>\n",
       "      <th>9905</th>\n",
       "      <th>9906</th>\n",
       "      <th>9907</th>\n",
       "      <th>9908</th>\n",
       "      <th>9909</th>\n",
       "      <th>9910</th>\n",
       "      <th>9911</th>\n",
       "      <th>9912</th>\n",
       "      <th>9913</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'quoteID': '2019-04-14-028425', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-12-006214', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-01-064419', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-09-075194', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-26-057136', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-15-036313', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-25-060160', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-02-073771', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-28-038981', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-03-005994', 'quotation': ...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'quoteID': '2019-04-03-007529', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-11-009600', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-11-078151', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-01-088752', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-12-052424', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-05-099481', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-19-004730', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-09-015136', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-05-022314', 'quotation': ...</td>\n",
       "      <td>{'quoteID': '2019-04-07-025206', 'quotation': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 9914 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                0     \\\n",
       "0  {'quoteID': '2019-04-14-028425', 'quotation': ...   \n",
       "\n",
       "                                                1     \\\n",
       "0  {'quoteID': '2019-04-12-006214', 'quotation': ...   \n",
       "\n",
       "                                                2     \\\n",
       "0  {'quoteID': '2019-04-01-064419', 'quotation': ...   \n",
       "\n",
       "                                                3     \\\n",
       "0  {'quoteID': '2019-04-09-075194', 'quotation': ...   \n",
       "\n",
       "                                                4     \\\n",
       "0  {'quoteID': '2019-04-26-057136', 'quotation': ...   \n",
       "\n",
       "                                                5     \\\n",
       "0  {'quoteID': '2019-04-15-036313', 'quotation': ...   \n",
       "\n",
       "                                                6     \\\n",
       "0  {'quoteID': '2019-04-25-060160', 'quotation': ...   \n",
       "\n",
       "                                                7     \\\n",
       "0  {'quoteID': '2019-04-02-073771', 'quotation': ...   \n",
       "\n",
       "                                                8     \\\n",
       "0  {'quoteID': '2019-04-28-038981', 'quotation': ...   \n",
       "\n",
       "                                                9     ...  \\\n",
       "0  {'quoteID': '2019-04-03-005994', 'quotation': ...  ...   \n",
       "\n",
       "                                                9904  \\\n",
       "0  {'quoteID': '2019-04-03-007529', 'quotation': ...   \n",
       "\n",
       "                                                9905  \\\n",
       "0  {'quoteID': '2019-04-11-009600', 'quotation': ...   \n",
       "\n",
       "                                                9906  \\\n",
       "0  {'quoteID': '2019-04-11-078151', 'quotation': ...   \n",
       "\n",
       "                                                9907  \\\n",
       "0  {'quoteID': '2019-04-01-088752', 'quotation': ...   \n",
       "\n",
       "                                                9908  \\\n",
       "0  {'quoteID': '2019-04-12-052424', 'quotation': ...   \n",
       "\n",
       "                                                9909  \\\n",
       "0  {'quoteID': '2019-04-05-099481', 'quotation': ...   \n",
       "\n",
       "                                                9910  \\\n",
       "0  {'quoteID': '2019-04-19-004730', 'quotation': ...   \n",
       "\n",
       "                                                9911  \\\n",
       "0  {'quoteID': '2019-04-09-015136', 'quotation': ...   \n",
       "\n",
       "                                                9912  \\\n",
       "0  {'quoteID': '2019-04-05-022314', 'quotation': ...   \n",
       "\n",
       "                                                9913  \n",
       "0  {'quoteID': '2019-04-07-025206', 'quotation': ...  \n",
       "\n",
       "[1 rows x 9914 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('./data/data_nlp/TEST2.json.bz2', orient='records', lines=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'0': '2019-04-14-028425', '1': '2019-04-12-00...</td>\n",
       "      <td>{'0': 'It's been a real kick-starter for great...</td>\n",
       "      <td>{'0': 'Lena Headey', '1': 'None', '2': 'Jeff Y...</td>\n",
       "      <td>{'0': ['Q228789'], '1': [], '2': ['Q6175309'],...</td>\n",
       "      <td>{'0': 1555236000000, '1': 1555027200000, '2': ...</td>\n",
       "      <td>{'0': 25, '1': 9, '2': 1, '3': 1, '4': 1, '5':...</td>\n",
       "      <td>{'0': [['Lena Headey', '0.922'], ['None', '0.0...</td>\n",
       "      <td>{'0': ['https://www.news.com.au/entertainment/...</td>\n",
       "      <td>{'0': 'E', '1': 'E', '2': 'E', '3': 'E', '4': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             quoteID  \\\n",
       "0  {'0': '2019-04-14-028425', '1': '2019-04-12-00...   \n",
       "\n",
       "                                           quotation  \\\n",
       "0  {'0': 'It's been a real kick-starter for great...   \n",
       "\n",
       "                                             speaker  \\\n",
       "0  {'0': 'Lena Headey', '1': 'None', '2': 'Jeff Y...   \n",
       "\n",
       "                                                qids  \\\n",
       "0  {'0': ['Q228789'], '1': [], '2': ['Q6175309'],...   \n",
       "\n",
       "                                                date  \\\n",
       "0  {'0': 1555236000000, '1': 1555027200000, '2': ...   \n",
       "\n",
       "                                      numOccurrences  \\\n",
       "0  {'0': 25, '1': 9, '2': 1, '3': 1, '4': 1, '5':...   \n",
       "\n",
       "                                              probas  \\\n",
       "0  {'0': [['Lena Headey', '0.922'], ['None', '0.0...   \n",
       "\n",
       "                                                urls  \\\n",
       "0  {'0': ['https://www.news.com.au/entertainment/...   \n",
       "\n",
       "                                               phase  \n",
       "0  {'0': 'E', '1': 'E', '2': 'E', '3': 'E', '4': ...  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF = pd.read_json('./data/data_nlp/TEST2.json.bz2', lines=True, compression='bz2')\n",
    "DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('./data/data_nlp/test.json.bz2', orient='records', lines=True, compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-04-14-028425</td>\n",
       "      <td>It's been a real kick-starter for great roles ...</td>\n",
       "      <td>Lena Headey</td>\n",
       "      <td>[Q228789]</td>\n",
       "      <td>2019-04-14 10:00:00</td>\n",
       "      <td>25</td>\n",
       "      <td>[[Lena Headey, 0.922], [None, 0.078]]</td>\n",
       "      <td>[https://www.news.com.au/entertainment/tv/game...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-04-12-006214</td>\n",
       "      <td>and warmly welcomes everyone to the club regar...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2019-04-12 00:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>[[None, 0.521], [Raelene Castle, 0.4254], [Bil...</td>\n",
       "      <td>[http://www.readingeagle.com/ap/article/folau-...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-04-01-064419</td>\n",
       "      <td>My thoughts and prayers are with the family me...</td>\n",
       "      <td>Jeff Yurek</td>\n",
       "      <td>[Q6175309]</td>\n",
       "      <td>2019-04-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Jeff Yurek, 0.9581], [None, 0.0419]]</td>\n",
       "      <td>[https://www.woodstocksentinelreview.com/news/...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-04-09-075194</td>\n",
       "      <td>Race, color, national origin, religion, sex, f...</td>\n",
       "      <td>Governor Kim Reynolds</td>\n",
       "      <td>[Q6409269]</td>\n",
       "      <td>2019-04-09 22:14:04</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Governor Kim Reynolds, 0.9014], [None, 0.0986]]</td>\n",
       "      <td>[https://kwwl.com/news/top-stories/2019/04/09/...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-04-26-057136</td>\n",
       "      <td>The other topics, I would be lying if I told y...</td>\n",
       "      <td>Moses Moreno</td>\n",
       "      <td>[Q6915887]</td>\n",
       "      <td>2019-04-26 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Moses Moreno, 0.7883], [None, 0.1616], [Andr...</td>\n",
       "      <td>[http://kunr.org/post/will-washoe-have-new-sex...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>2019-04-05-099481</td>\n",
       "      <td>There's a stigma that people are bad, somehow,...</td>\n",
       "      <td>John Herrington</td>\n",
       "      <td>[Q45082]</td>\n",
       "      <td>2019-04-05 23:12:30</td>\n",
       "      <td>1</td>\n",
       "      <td>[[John Herrington, 0.7944], [None, 0.2056]]</td>\n",
       "      <td>[http://www.wdam.com/2019/04/05/pine-belt-wome...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9910</th>\n",
       "      <td>2019-04-19-004730</td>\n",
       "      <td>blatantly mocked survivors of sexual assault a...</td>\n",
       "      <td>Camille Paglia</td>\n",
       "      <td>[Q255463]</td>\n",
       "      <td>2019-04-19 20:52:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Camille Paglia, 0.7991], [None, 0.2009]]</td>\n",
       "      <td>[https://www.pinknews.co.uk/2019/04/19/camille...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9911</th>\n",
       "      <td>2019-04-09-015136</td>\n",
       "      <td>Clean, half-brogue, Oxford monkstrap and whole...</td>\n",
       "      <td>Edward Sexton</td>\n",
       "      <td>[Q5345282]</td>\n",
       "      <td>2019-04-09 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Edward Sexton, 0.686], [None, 0.314]]</td>\n",
       "      <td>[https://robbreport.com/style/footwear/brown-s...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9912</th>\n",
       "      <td>2019-04-05-022314</td>\n",
       "      <td>For example, the JAS specialists will carefull...</td>\n",
       "      <td>Mark Speakman</td>\n",
       "      <td>[Q6769797]</td>\n",
       "      <td>2019-04-05 12:31:08</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Mark Speakman, 0.4881], [Mr Ward, 0.345], [N...</td>\n",
       "      <td>[https://www.miragenews.com/statewide-justice-...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9913</th>\n",
       "      <td>2019-04-07-025206</td>\n",
       "      <td>in an era defined by the #MeToo movement,</td>\n",
       "      <td>Nancy Gibbs</td>\n",
       "      <td>[Q6962706]</td>\n",
       "      <td>2019-04-07 12:16:42</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Nancy Gibbs, 0.5951], [None, 0.3056], [Joe B...</td>\n",
       "      <td>[https://www.cnn.com/2019/04/07/opinions/trump...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9914 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                quoteID                                          quotation  \\\n",
       "0     2019-04-14-028425  It's been a real kick-starter for great roles ...   \n",
       "1     2019-04-12-006214  and warmly welcomes everyone to the club regar...   \n",
       "2     2019-04-01-064419  My thoughts and prayers are with the family me...   \n",
       "3     2019-04-09-075194  Race, color, national origin, religion, sex, f...   \n",
       "4     2019-04-26-057136  The other topics, I would be lying if I told y...   \n",
       "...                 ...                                                ...   \n",
       "9909  2019-04-05-099481  There's a stigma that people are bad, somehow,...   \n",
       "9910  2019-04-19-004730  blatantly mocked survivors of sexual assault a...   \n",
       "9911  2019-04-09-015136  Clean, half-brogue, Oxford monkstrap and whole...   \n",
       "9912  2019-04-05-022314  For example, the JAS specialists will carefull...   \n",
       "9913  2019-04-07-025206          in an era defined by the #MeToo movement,   \n",
       "\n",
       "                    speaker        qids                date  numOccurrences  \\\n",
       "0               Lena Headey   [Q228789] 2019-04-14 10:00:00              25   \n",
       "1                      None          [] 2019-04-12 00:00:00               9   \n",
       "2                Jeff Yurek  [Q6175309] 2019-04-01 00:00:00               1   \n",
       "3     Governor Kim Reynolds  [Q6409269] 2019-04-09 22:14:04               1   \n",
       "4              Moses Moreno  [Q6915887] 2019-04-26 00:00:00               1   \n",
       "...                     ...         ...                 ...             ...   \n",
       "9909        John Herrington    [Q45082] 2019-04-05 23:12:30               1   \n",
       "9910         Camille Paglia   [Q255463] 2019-04-19 20:52:00               1   \n",
       "9911          Edward Sexton  [Q5345282] 2019-04-09 00:00:00               1   \n",
       "9912          Mark Speakman  [Q6769797] 2019-04-05 12:31:08               1   \n",
       "9913            Nancy Gibbs  [Q6962706] 2019-04-07 12:16:42               1   \n",
       "\n",
       "                                                 probas  \\\n",
       "0                 [[Lena Headey, 0.922], [None, 0.078]]   \n",
       "1     [[None, 0.521], [Raelene Castle, 0.4254], [Bil...   \n",
       "2                [[Jeff Yurek, 0.9581], [None, 0.0419]]   \n",
       "3     [[Governor Kim Reynolds, 0.9014], [None, 0.0986]]   \n",
       "4     [[Moses Moreno, 0.7883], [None, 0.1616], [Andr...   \n",
       "...                                                 ...   \n",
       "9909        [[John Herrington, 0.7944], [None, 0.2056]]   \n",
       "9910         [[Camille Paglia, 0.7991], [None, 0.2009]]   \n",
       "9911            [[Edward Sexton, 0.686], [None, 0.314]]   \n",
       "9912  [[Mark Speakman, 0.4881], [Mr Ward, 0.345], [N...   \n",
       "9913  [[Nancy Gibbs, 0.5951], [None, 0.3056], [Joe B...   \n",
       "\n",
       "                                                   urls phase  \n",
       "0     [https://www.news.com.au/entertainment/tv/game...     E  \n",
       "1     [http://www.readingeagle.com/ap/article/folau-...     E  \n",
       "2     [https://www.woodstocksentinelreview.com/news/...     E  \n",
       "3     [https://kwwl.com/news/top-stories/2019/04/09/...     E  \n",
       "4     [http://kunr.org/post/will-washoe-have-new-sex...     E  \n",
       "...                                                 ...   ...  \n",
       "9909  [http://www.wdam.com/2019/04/05/pine-belt-wome...     E  \n",
       "9910  [https://www.pinknews.co.uk/2019/04/19/camille...     E  \n",
       "9911  [https://robbreport.com/style/footwear/brown-s...     E  \n",
       "9912  [https://www.miragenews.com/statewide-justice-...     E  \n",
       "9913  [https://www.cnn.com/2019/04/07/opinions/trump...     E  \n",
       "\n",
       "[9914 rows x 9 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF = pd.read_json('./data/data_nlp/test.json.bz2', lines=True, compression='bz2')\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ddbd0b842a978e03471eb3a4ae18fdd24eb8ad76bdab23b363108c4c8f6a59c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('ada': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
